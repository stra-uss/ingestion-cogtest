{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                |                                                                                         |  \n",
    "| -------------  |:----------------------------------------------------------------------------------------|\n",
    "| Autor          | Strauss Cunha Carvalho                                                                  |\n",
    "| e-Mail         | engsts@gmail.com                                                                        |\n",
    "| Data           | 2020/06/27                                                                              |  \n",
    "| Git            | https://github.com/stra-uss/ingestion-cogtest                                           |\n",
    "| Linkedin       | https://www.linkedin.com/in/strauss-cunha-carvalho-1a601a15/                            |\n",
    "| Lattes         | http://buscatextual.cnpq.br/buscatextual/visualizacv.do?metodo=apresentar&id=K4226288E5 |             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisitos\n",
    "1. Conversão do formato dos arquivos: Converter o arquivo CSV presente no diretório data/input/users/load.csv, para um formato colunar de alta performance de leitura de sua escolha. Justificar brevemente a escolha do formato;\n",
    "\n",
    "2. Deduplicação dos dados convertidos: No conjunto de dados convertidos haverão múltiplas entradas para um mesmo registro, variando apenas os valores de alguns dos campos entre elas. Será necessário realizar um processo de deduplicação destes dados, a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados e a data de atualização (update_date) para definição do registro mais recente;\n",
    "\n",
    "3. Conversão do tipo dos dados deduplicados: No diretório config haverá um arquivo JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output. Utilizando esse arquivo como input, realizar um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados;\n",
    "\n",
    "### Notas gerais\n",
    "- Todas as operações devem ser realizadas utilizando Spark. O serviço de execução fica a seu critério, podendo utilizar tanto serviços locais como serviços em cloud. Justificar brevemente o serviço escolhido (EMR, Glue, Zeppelin, etc.).\n",
    "\n",
    "- Cada operação deve ser realizada no dataframe resultante do passo anterior, podendo ser persistido e carregado em diferentes conjuntos de arquivos após cada etapa ou executados em memória e apenas persistido após operação final.\n",
    "\n",
    "- Você tem liberdade p/ seguir a sequência de execução desejada;\n",
    "\n",
    "- Solicitamos a transformação de tipos de dados apenas de alguns campos. Os outros ficam a seu critério\n",
    "\n",
    "- O arquivo ou o conjunto de arquivos finais devem ser compactados e enviados por e-mail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a realização do teste, optou-se por utilizar um ambiente local, devido às instalações das dependências das libs do Pyspark envolvidas e à  otimização do tempo de desenvolvimento.\n",
    "\n",
    "Na Tabela a seguir, apresenta-se um resumo das tecnologias empregadas e suas respectivas versões.\n",
    "\n",
    "| Tecnologias                | Versão                         |   \n",
    "| ---------------------------|:-------------------------------|\n",
    "| Spark                      | 2.4.3                          |\n",
    "| Hadoop                     | 3.1.2                          |\n",
    "| Zeppelin                   | 0.8.1                          |\n",
    "| Sistema operacional local  | Ubuntu Linux 4.15.0-99-generic |\n",
    "\n",
    "\n",
    "Obs: devido à renderização do notebook gerado no Apache Zeppelin não apresentar uma visualização adequada no GitHub, o código fonte foi convertido para:\n",
    "1 - o formato do Jupyter (conforme repositório Git) https://github.com/stra-uss/ingestion-cogtest; e \n",
    "2 - .pdf (em anexo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType  \n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Environment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "local  = '/media/oak/4D38-34DE/Biblioteca/jobs/teste_cognitivo_ai/teste-eng-dados'\n",
    "inp = '/data/input/users/' \n",
    "out = '/data/output/'\n",
    "cfg = '/config/'\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "file_csv = 'load.csv'\n",
    "df_csv_path = local + inp + file_csv\n",
    "df_raw = spark.read.csv(df_csv_path, escape='\"', multiLine=True,inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Dataset raw (.csv) schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.2 - Data Type changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1) Reading the schema from .json file and altering data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "file_sch = 'types_mapping.json'\n",
    "with open(local + cfg + file_sch) as f:\n",
    "    schema = StructType.fromJson(json.load(f))\n",
    "\n",
    "struct=StructType(fields=schema)\n",
    "df_user =spark.read.csv(df_csv_path,schema=struct)\n",
    "df_user = df_user.na.drop()    \n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_user.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Dataset user preview (with altered types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "df_user.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Dataset conversion (csv to parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grosso modo, entre as duas opções mais tradicionais de formato de arquivos colunares presentes no ambiente Hadoop - ORC e o Parquet - optou-se, nesta POC, pelo uso do formato Parquet. \n",
    "Além de satisfazer os requisitos do Teste, citam-se dois motivos básicos pela escolha:\n",
    "1) Não há menção às operações de atualização ou escrita no dataframe resultante; e\n",
    "2) Não há menção à necessidade de compactação do dataframe resultante, o que, nesta caso, viabilizaria o uso do formato Orc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "file_pqt = 'load.parquet'\n",
    "df_pqt_path = local + out + file_pqt\n",
    "df_user.write.parquet(df_pqt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Dataset deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 - Read Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df_pqt = spark.read.parquet(df_pqt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - Dataset (parquet) Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df_pqt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 - Dataset (parquet) Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "df_pqt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 - Remove duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "window = Window.partitionBy('id').orderBy(df_pqt['update_date'].desc())\n",
    "df_pqt = df_pqt.withColumn(\"last_update\",rank().over(window))\n",
    "df_final = df_pqt.filter(\"last_update=1\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Final dataset storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "file_pqt = 'final.parquet'\n",
    "df_pqt_path = local + out + file_pqt\n",
    "df_final.write.parquet(df_pqt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0",
   "language": "python",
   "name": "spark2"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "scala",
   "pygments_lexer": "python",
   "version": "3.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
